{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compatible-clearance",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-display",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-inspection",
   "metadata": {},
   "source": [
    "# Run token analysis and topic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "welsh-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import warnings\n",
    "import math\n",
    "import numpy\n",
    "import numpy as np\n",
    "import collections\n",
    "import string\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "\n",
    "# preprocessing method\n",
    "def string_tokenise(string):  # return list\n",
    "    result = re.findall(r\"\\w+\", string)\n",
    "    return result\n",
    "\n",
    "\n",
    "def case_fold(list1):  # return list\n",
    "    result = [word.lower() for word in list1]\n",
    "    #     string = ' '.join([str(elem) for elem in list1])\n",
    "    #     result = string.lower().split() #lower() is the same as casefold()\n",
    "    return result\n",
    "\n",
    "\n",
    "def stopping(list1):  # return list\n",
    "    stopfile = open(\"englishST.txt\", 'r')\n",
    "    stopwords = stopfile.read().split()\n",
    "    result = [items for items in list1 if items not in stopwords]\n",
    "    return result\n",
    "\n",
    "\n",
    "def normalise(list1):  # return list\n",
    "    result = []\n",
    "    for item in list1:\n",
    "        result.append(stem(item))\n",
    "    return result\n",
    "\n",
    "\n",
    "def pre_process(string):  # return list\n",
    "    result = normalise(stopping(case_fold(string_tokenise(string))))\n",
    "    return result\n",
    "\n",
    "\n",
    "# find out the terms that appear >= 10 times\n",
    "# input: the name of the corpus | return: a list including the freq term\n",
    "def find_freq_terms10(corpus_name):\n",
    "    # put all tokens into a single list\n",
    "    all_tokens = []\n",
    "    for name, content in verses_list:  # e.g. verses_list[0] == ['Quran', ['prais', 'allah', 'lord', 'world']]\n",
    "        if name == corpus_name:\n",
    "            all_tokens += content\n",
    "\n",
    "    count = Counter(all_tokens)  # count the how many times the terms appears, Counter(['prais'])['prais'] == 1\n",
    "    freq_terms_list = []\n",
    "    freq_terms_list.append(corpus_name)  # put corpus name at the first one in the list\n",
    "    for term in count:\n",
    "        if count[term] >= 10:  # if the term appering more than 10 times\n",
    "            freq_terms_list.append(term)\n",
    "    return freq_terms_list\n",
    "\n",
    "\n",
    "# calculate MI\n",
    "def MI(N11, N01, N10, N00):\n",
    "    N = N11 + N01 + N10 + N00\n",
    "    N1_ = N11 + N10  # N1.\n",
    "    N_1 = N11 + N01  # N.1\n",
    "    N0_ = N00 + N01  # N0.\n",
    "    N_0 = N00 + N10  # N.0\n",
    "    # To avoid zero division\n",
    "    output = 0\n",
    "    if N11 != 0:\n",
    "        output += (N11 / N) * np.log2((N * N11) / (N1_ * N_1))\n",
    "    if N01 != 0:\n",
    "        output += (N01 / N) * np.log2((N * N01) / (N0_ * N_1))\n",
    "    if N10 != 0:\n",
    "        output += (N10 / N) * np.log2((N * N10) / (N1_ * N_0))\n",
    "    if N01 != 0:\n",
    "        output += (N00 / N) * np.log2((N * N00) / (N0_ * N_0))\n",
    "    return output\n",
    "\n",
    "\n",
    "# Calculate Chi_squared\n",
    "def Chi_squared(N11, N01, N10, N00):\n",
    "    N = N11 + N01 + N10 + N00\n",
    "    return (N * ((N11 * N00 - N10 * N01) ** 2)) / ((N11 + N01) * (N11 + N10) * (N10 + N00) * (N01 + N00))\n",
    "\n",
    "\n",
    "verses_list = []\n",
    "# a list to store the preprocessed corpora in list structure, e.g.['Quran', ['prais', 'allah', 'lord', 'world']]\n",
    "\n",
    "Poynter = open('PoynterCovid19Database_Reference_Article.csv', 'r')\n",
    "englishST = open('englishST.txt', 'r')  # read stop words\n",
    "corpora = csv.reader(Poynter, delimiter=',')\n",
    "stopword_list = englishST.read().split()  # read stop words into a list\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "for (docID,content,accuracy,date,region,explanation,\n",
    "     reference_url,reference_html,reference_text) in corpora:\n",
    "    p_content = pre_process(explanation)\n",
    "    verses_list.append(['Poynter', p_content])\n",
    "    # strore preprocessed corpora in this list structure, e.g.['Quran', ['prais', 'allah', 'lord', 'world']]\n",
    "\n",
    "# find terms that appears >= 10 times and put them in to one list\n",
    "Quran_terms = find_freq_terms10(corpus_name='Quran')  # NB the first element is the name of corpus\n",
    "OT_terms = find_freq_terms10(corpus_name='OT')  # NB the first element is the name of corpus\n",
    "NT_terms = find_freq_terms10(corpus_name='NT')  # NB the first element is the name of corpus\n",
    "Poynter_term = find_freq_terms10(corpus_name='Poynter')\n",
    "corpora_terms = [Poynter_term]\n",
    "# put in to one list [['Quran',<Quran_terms>],[<'OT',OT_terms>],['NT',<NT_terms>]]\n",
    "\n",
    "# identify top 10 highest scoring words for each method for each corpus\n",
    "# for corpus_terms in corpora_terms:\n",
    "#     # for the frequently appearing terms of each corpus. corpus_terms is a list likes [<Quran_terms>]\n",
    "#     corpus_of_terms = corpus_terms[0]  # recall the first element is the name of the corpus\n",
    "#     MI_dict = {}  # a dictionary with structure {term:MI}\n",
    "#     Chi_dict = {}  # a dictionary with structure {term:Chi Squared}\n",
    "#     for term in corpus_terms[1:]:\n",
    "#         N11 = 0\n",
    "#         N01 = 0\n",
    "#         N10 = 0\n",
    "#         N00 = 0\n",
    "#         for corpus_of_docs, p_content in verses_list:\n",
    "#             if corpus_of_terms == corpus_of_docs:\n",
    "#                 # number of documents that contain the word are in the class (N11)\n",
    "#                 if term in p_content:\n",
    "#                     N11 = N11 + 1\n",
    "#                 # number of documents that don't contain the word are in the class(N01)\n",
    "#                 else:\n",
    "#                     N01 = N01 + 1\n",
    "#             else:\n",
    "#                 # number of documents where this term present in other corpora(N10)\n",
    "#                 if term in p_content:\n",
    "#                     N10 = N10 + 1\n",
    "#                 # number of documents where this term does not present in other corpora(N00)\n",
    "#                 else:\n",
    "#                     N00 = N00 + 1\n",
    "#         MI_dict[term] = MI(N11, N01, N10, N00)\n",
    "#         Chi_dict[term] = Chi_squared(N11, N01, N10, N00)\n",
    "\n",
    "#     # sort the dictionary\n",
    "#     sorted_MI_dict = sorted(MI_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "#     sorted_Chi_dict = sorted(Chi_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "#     print('The top 10 highest Mutual Information score of ' + corpus_of_terms)\n",
    "#     print(sorted_MI_dict[:10])\n",
    "#     print('\\n')\n",
    "#     print('The top 10 highest Chi_squared score of ' + corpus_of_terms)\n",
    "#     print(sorted_Chi_dict[:10])\n",
    "#     print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "italic-smart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs_topic_score_list of Poynter\n",
      "[18.          0.07362767]\n",
      "0.061*\"coronavirus\" + 0.050*\"covid\" + 0.026*\"vaccin\" + 0.025*\"virus\" + 0.017*\"flu\" + 0.017*\"infect\" + 0.014*\"evid\" + 0.013*\"cure\" + 0.010*\"diseas\" + 0.009*\"studi\"\n",
      "[13.          0.07190815]\n",
      "0.050*\"covid\" + 0.040*\"evid\" + 0.024*\"coronavirus\" + 0.022*\"cure\" + 0.019*\"drug\" + 0.018*\"prevent\" + 0.017*\"diseas\" + 0.017*\"scientif\" + 0.016*\"health\" + 0.015*\"virus\"\n",
      "[5.        0.0703799]\n",
      "0.030*\"polic\" + 0.026*\"fals\" + 0.021*\"messag\" + 0.015*\"govern\" + 0.013*\"video\" + 0.012*\"inform\" + 0.011*\"peopl\" + 0.010*\"covid\" + 0.009*\"issu\" + 0.009*\"citi\"\n",
      "[2.         0.06900531]\n",
      "0.044*\"video\" + 0.041*\"photo\" + 0.039*\"show\" + 0.025*\"coronavirus\" + 0.017*\"covid\" + 0.012*\"imag\" + 0.011*\"2019\" + 0.010*\"2018\" + 0.010*\"anim\" + 0.010*\"vaccin\"\n",
      "[6.         0.05857888]\n",
      "0.059*\"20\" + 0.038*\"coronavirus\" + 0.030*\"3\" + 0.028*\"video\" + 0.022*\"2019\" + 0.019*\"imag\" + 0.018*\"2\" + 0.010*\"current\" + 0.010*\"covid\" + 0.010*\"ncov\"\n",
      "[17.          0.05780697]\n",
      "0.064*\"video\" + 0.025*\"doctor\" + 0.019*\"coronavirus\" + 0.018*\"hospit\" + 0.016*\"presid\" + 0.015*\"covid\" + 0.012*\"patient\" + 0.012*\"2016\" + 0.011*\"fals\" + 0.011*\"audio\"\n",
      "[10.          0.05477184]\n",
      "0.056*\"coronavirus\" + 0.029*\"health\" + 0.024*\"evid\" + 0.022*\"covid\" + 0.016*\"cure\" + 0.015*\"drink\" + 0.014*\"support\" + 0.013*\"prevent\" + 0.013*\"organ\" + 0.013*\"water\"\n",
      "[11.          0.05369112]\n",
      "0.034*\"test\" + 0.031*\"coronavirus\" + 0.021*\"chines\" + 0.021*\"case\" + 0.014*\"china\" + 0.013*\"medic\" + 0.013*\"confirm\" + 0.013*\"report\" + 0.013*\"covid\" + 0.010*\"health\"\n",
      "[12.          0.05119093]\n",
      "0.038*\"offici\" + 0.027*\"fake\" + 0.022*\"deni\" + 0.021*\"account\" + 0.020*\"inform\" + 0.015*\"statement\" + 0.014*\"websit\" + 0.014*\"vitamin\" + 0.014*\"graphic\" + 0.013*\"fals\"\n",
      "[8.         0.04588141]\n",
      "0.023*\"case\" + 0.020*\"coronavirus\" + 0.018*\"imag\" + 0.016*\"health\" + 0.015*\"ministri\" + 0.014*\"photo\" + 0.014*\"covid\" + 0.013*\"manipul\" + 0.012*\"book\" + 0.012*\"time\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement IDA, identify the topic that has the highest average score for the three corpora (3 topics).\n",
    "# For each of those three topics, find the top 10 tokens with highest probability of belonging to that topic.\n",
    "common_texts = [item[1] for item in verses_list]\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=20, random_state=1000, id2word=common_dictionary)\n",
    "# get the topic scores for each document\n",
    "docs_topic_score_list = lda.get_document_topics(bow=common_corpus, minimum_probability=0.00)\n",
    "\n",
    "# identify top3 topics\n",
    "for corpus_name in ['Poynter']:\n",
    "    # compute the topic probabilities\n",
    "    topic_prob = []  # topic_prob contains the topic prob for each document in one corpus\n",
    "    for verse, prob in zip(verses_list, docs_topic_score_list):\n",
    "        if verse[0] == corpus_name:  # eg of verse: ['Quran', ['prais', 'allah', 'lord', 'world']], where verses[\n",
    "            # 0]=='Quran'\n",
    "            topic_prob.append(prob)  # add the topic prob for one doc into the list\n",
    "    print('docs_topic_score_list of ' + corpus_name)\n",
    "    # compute overall topic probabilities for each corpus by averaging the\n",
    "    # topic probabilities for all documents belonging to each corpus\n",
    "    overall_topic_prob = np.mean(topic_prob, axis=0)\n",
    "    top3_topics = sorted(overall_topic_prob, reverse=True, key=lambda x: x[1])[\n",
    "                  :10]  # sort the overall_topic_prob to identify top3\n",
    "    for topic in top3_topics:  # eg of topic: array([19.        ,  0.08826874])\n",
    "        print(topic)\n",
    "        print(lda.print_topic(\n",
    "            int(topic[0])))  # print the the top 10 tokens and their probability scores for each of the 3 topics\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "420px",
    "left": "677px",
    "right": "20px",
    "top": "120px",
    "width": "320px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
