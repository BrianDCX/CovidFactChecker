{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "physical-greek",
   "metadata": {},
   "source": [
    "# Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-share",
   "metadata": {},
   "source": [
    "This is a baseline model, which only uses BM25+ with pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-turtle",
   "metadata": {},
   "source": [
    "# Preparation and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "welsh-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import warnings\n",
    "import math\n",
    "from stemming.porter2 import stem\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Plus\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# preprocessing method\n",
    "def string_tokenise(string):  # return list\n",
    "    result = re.findall(r\"\\w+\", string)\n",
    "    return result\n",
    "\n",
    "\n",
    "def case_fold(list1):  # return list\n",
    "    result = [word.lower() for word in list1]\n",
    "    #     string = ' '.join([str(elem) for elem in list1])\n",
    "    #     result = string.lower().split() #lower() is the same as casefold()\n",
    "    return result\n",
    "\n",
    "\n",
    "def stopping(list1):  # return list\n",
    "    stopfile = open(\"englishST.txt\", 'r')\n",
    "    stopwords = stopfile.read().split()\n",
    "    result = [items for items in list1 if items not in stopwords]\n",
    "    return result\n",
    "\n",
    "\n",
    "def normalise(list1):  # return list\n",
    "    result = []\n",
    "    for item in list1:\n",
    "        result.append(stem(item))\n",
    "    return result\n",
    "\n",
    "\n",
    "'''\n",
    "=======================================Preparation: indexing======================================\n",
    "'''\n",
    "\n",
    "englishST = open('englishST.txt', 'r')\n",
    "\n",
    "FILE = 'v3.0/verified_claims.docs.tsv'\n",
    "\n",
    "record = {}  # {{string:{int:string}}} {{term:{docID:position}}}dic of dic, every insider dic records a term\n",
    "docID_list = []  # list just for recording docID\n",
    "\n",
    "fields = ['vclaim_id', 'vclaim', 'title']\n",
    "dataframe = pd.read_csv(FILE, usecols = fields, sep = '\\t')\n",
    "\n",
    "docID = 0\n",
    "\n",
    "preprocessed_corpus = [] #a list of lists of strings, which are the document tokens\n",
    "\n",
    "for index, row in dataframe.iterrows():\n",
    "\n",
    "    docID = row['vclaim_id']\n",
    "    docID_list.append(docID)\n",
    "\n",
    "    # step1: tokenise\n",
    "    doc_in_str = row['vclaim'] # doc is in string format\n",
    "    pos_in_doc = 0  # describe the position of terms in one doc\n",
    "\n",
    "    term_in_list = normalise(\n",
    "        stopping(case_fold(string_tokenise(doc_in_str))))  # pre-processing\n",
    "\n",
    "    preprocessed_corpus.append(term_in_list) # add a list of cocument terms into the list of list of doc terms\n",
    "\n",
    "\n",
    "bm25_indices = BM25Plus(preprocessed_corpus)\n",
    "\n",
    "# np.save('03_BM25indices.npy', bm25_indices) # save the indices to avoid reproduce it every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-carolina",
   "metadata": {},
   "source": [
    "# Implement retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "military-newport",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-64f8ccd39a3b>:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if vclaim_scores == []:\n"
     ]
    }
   ],
   "source": [
    "# load the BM25Plus index data\n",
    "# bm25_indices = np.load('03_BM25indices.npy', allow_pickle=True).copy\n",
    "\n",
    "'''\n",
    "========================================Document Retrieval=======================================\n",
    "'''\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "\n",
    "RankedIROutput = open('04_results.tsv', 'w')\n",
    "results_fields = ['tweet_id','Q0','vclaim_id','rank','score','tag']\n",
    "writer = csv.DictWriter(RankedIROutput, fieldnames = results_fields)\n",
    "writer.writeheader()\n",
    "\n",
    "# NO HEADER NEEDED!!\n",
    "\n",
    "# step1: extract the original queries from file\n",
    "tweets_directory = 'v3.0/train/tweets.queries.tsv'\n",
    "tweets_fields = ['tweet_id', 'tweet_content']\n",
    "df_t = pd.read_csv(tweets_directory, usecols = tweets_fields, sep = '\\t')\n",
    "tweets_list = df_t.tweet_content.tolist().copy()\n",
    "tweets_id = df_t.tweet_id.tolist().copy()\n",
    "\n",
    "\n",
    "for query_id, query in zip(tweets_id,tweets_list):\n",
    "    query_term = normalise(\n",
    "        stopping(case_fold(string_tokenise(query))))  # same preprocessing as for indexing\n",
    "    vclaim_scores = bm25_indices.get_scores(query_term)\n",
    "    \n",
    "    search_result = sorted(zip(docID_list, vclaim_scores),key = itemgetter(1), reverse=True) # sorted by the second column in the tuple\n",
    "\n",
    "    # handle the case that cannot find any matches\n",
    "    if vclaim_scores == []:\n",
    "        search_result = []\n",
    "\n",
    "    # write into submitted file\n",
    "    count = 0  # provide up to 150 result\n",
    "    for matched_docID, matched_score in search_result:\n",
    "        count = count + 1\n",
    "        if count > 30: # output top 1 result\n",
    "            break\n",
    "            \n",
    "#         RankedIROutput.write(str(query_id) + ',' + str(matched_entry[0]) + ',' + str(round(matched_entry[1],4)) +\n",
    "#                             ' ||' + dataframe.loc[dataframe['docID']==matched_entry[0]]['content'].item() + '\\n')\n",
    "\n",
    "        tweet_id = query_id\n",
    "        vclaim_id = matched_docID\n",
    "        score = matched_score\n",
    "        tag = 'DC'\n",
    "        return_data = {'tweet_id':tweet_id, 'Q0':'Q0', 'vclaim_id':vclaim_id, 'rank': '1',\n",
    "                      'score': score, 'tag': tag}\n",
    "        writer = csv.DictWriter(RankedIROutput, fieldnames = results_fields, delimiter='\\t')\n",
    "        writer.writerow(return_data)\n",
    "        \n",
    "        \n",
    "RankedIROutput.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "unique-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3232323, 'zzdfdfdfdfd'), (10000, 'fdfdf'), (22, 'zerer')]\n"
     ]
    }
   ],
   "source": [
    "# experiment \n",
    "a = [22,10000,3232323]\n",
    "b = ['zerer','fdfdf','zzdfdfdfdfd']\n",
    "from operator import itemgetter\n",
    "print(sorted(zip(a,b), key = itemgetter(0), reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 500,
   "position": {
    "height": "522px",
    "left": "647px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
